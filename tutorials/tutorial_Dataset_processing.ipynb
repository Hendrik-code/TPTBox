{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ruamel.yaml configargparse spineps hf-deepali TypeSaveArgParse\n",
    "!pip install TPTBox --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This tutorial will give you an overview of how to get from a DICOM dump to a processed Dataset with segmentations.\n",
    "\n",
    "abbreviations:\n",
    "POI: Point of interest\n",
    "\n",
    "Steps:\n",
    "\n",
    "(1) Dicom export to BIDS dataset\n",
    "\n",
    "(2) Stitching\n",
    "\n",
    "(3) Segmentation TotalVibeSegmentator and Spineps\n",
    "\n",
    "(4) Points of Interest (POI) \n",
    "\n",
    "(5) Point registration\n",
    "\n",
    "(6) Deformable Registration for different breath holds compensation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dicom export to BIDS dataset\n",
    "\n",
    "Short overview:\n",
    "\n",
    "A BIDS dataset is a file naming convention.\n",
    "\n",
    "The following rules should be known and weakly enforced:\n",
    "\n",
    "- A dataset folder should start with 'dataset-{YOUR-NAME}'\n",
    "- The next level folders are:\n",
    "  - rawdata: for all imaging data.\n",
    "  - derivative: for all generated data, like segmentation.\n",
    "A file should look like:\n",
    "\n",
    "sub-{Subject name}_ses-{Session}_{key}-{value}*_{format}.{filetype}\n",
    "- Subject name: Unique identifier \n",
    "- Session: Session id. Optional if there is only one session\n",
    "- Any number of key/value pairs. Keys are unique. The defined keys are here: https://bids-specification.readthedocs.io/en/stable/appendices/entities.html . Our tool enforces a certain order. See tutorial_BIDS_files.ipynb\n",
    "- format: type of acquisition like ct, T2w, VIBE, MPRage\n",
    "Do not use '_' in any key or values. \n",
    "\n",
    "See https://bids-specification.readthedocs.io/en/stable/ for a detailed description of what BIDS is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox.core.bids_files import entities_keys, formats\n",
    "\n",
    "print(\"Known formats:\\n\", \"; \".join(formats))\n",
    "print()\n",
    "print()\n",
    "print(\"Order of keys we enforce:\\n\", \"; \".join(entities_keys.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "This function extracts a dicom folder and converts it to a BIDS-like Niffty folder.\n",
    "\n",
    "The names are created like this: DICOM:Key is given Dicom key\n",
    "      \n",
    "`dataset-{NAME}/rawdate/sub-{DICOM:PatientID}/ses-{DICOM:StudyDate}/{format}/sub-{DICOM:PatientID}_ses-{DICOM:StudyDate}_sequ-{DICOM:SeriesNumber}_acq-{sag|ax|cor|iso}_{format}.nii.gz`\n",
    "\n",
    "\n",
    "And a .json, where the DICOM-Keys are saved.\n",
    "\n",
    "To get {format} we use string matching and the dicom \"SeriesDescription\" key. As this is a free text, it will not always work. Then we default to \"mr,\" and you have to manually rename them.\n",
    "\n",
    "\n",
    "You can use make_subject_chunks = n [int] for a very large dataset. Then, we put an additional folder with the first n letters between the raw data and the sub-folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from TPTBox.segmentation.TotalVibeSeg.auto_download import _download\n",
    "\n",
    "## Download the Dicom Example\n",
    "path_to_dicom_dataset = Path(\"PixelPandemonium\").absolute()\n",
    "target_folder = Path(path_to_dicom_dataset).parent\n",
    "\n",
    "if not path_to_dicom_dataset.exists():\n",
    "    _download(\n",
    "        \"https://github.com/robert-graf/TotalVibeSegmentator/releases/download/example/PixelPandemonium.zip\",\n",
    "        path_to_dicom_dataset,\n",
    "        text=\"example\",\n",
    "    )\n",
    "\n",
    "dataset_name = Path(path_to_dicom_dataset).name.replace(\"_\", \"-\")  # TODO Remove\n",
    "dataset = target_folder / f\"dataset-{dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox.core.dicom.dicom_extract import extract_dicom_folder\n",
    "\n",
    "extract_dicom_folder(Path(path_to_dicom_dataset), dataset, use_session=True, n_cpu=10, skip_localizer=True)\n",
    "\n",
    "## Example for https://data.mendeley.com/datasets/k57fr854j2/2, where they annonomieced the subject, and we have to extract them from the filename/folder structure\n",
    "# extract_dicom_folder(Path(path_to_dicom_dataset), dataset,use_session=True,n_cpu=10,override_subject_name=lambda x,y: y.name.split(\"_\")[-2],skip_localizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping over a BIDS dataset\n",
    "Our tool can automatically scan BIDS folders and create a grouped dictionary where you can pick out the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import BIDS_FILE, BIDS_Global_info\n",
    "\n",
    "bgi = BIDS_Global_info(dataset, [\"rawdata\", \"rawdata_stiched\", \"derivative\"])\n",
    "stitching_candidate: list[BIDS_FILE] = []\n",
    "epsilon = 0.2\n",
    "for name, subj in bgi.iter_subjects():\n",
    "    print(\"Subject identifier\", name)\n",
    "    q = subj.new_query()\n",
    "    # Filter by some rules\n",
    "    q.flatten()\n",
    "    q.filter_filetype(\"nii.gz\")\n",
    "    q.unflatten()\n",
    "    for fam in q.loop_dict():\n",
    "        print(f\"New Family with {fam.keys()}\")\n",
    "        print(fam)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that we already grouped files into \"familes\" based on there BIDS-keys.\n",
    "\n",
    "If you just want to find all familes that have a sagittal inphase T2w image you can filter by the BIDS keys.\n",
    "\n",
    "If you produce a segmentation, the BIDS-Keys should in a way that the land in the same family as the source file. If you want to see this effect you can rerun this cell after we crated our segmentation.\n",
    "\n",
    "If you do not care about families you can flatten the looping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import BIDS_FILE, BIDS_Global_info\n",
    "\n",
    "bgi = BIDS_Global_info(dataset, [\"rawdata\", \"rawdata_stiched\", \"derivative\"])\n",
    "stitching_candidate: list[BIDS_FILE] = []\n",
    "epsilon = 0.2\n",
    "for name, subj in bgi.iter_subjects():\n",
    "    print(\"Subject identifier\", name)\n",
    "    q = subj.new_query()\n",
    "    # Filter by some rules\n",
    "    q.flatten()\n",
    "    q.filter_filetype(\"nii.gz\")\n",
    "    q.filter_format(\"T2w\")\n",
    "    q.filter(\"acq\", \"sag\")\n",
    "    q.filter(\"part\", \"inphase\")\n",
    "    for fam in q.loop_list():\n",
    "        print(fam)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example automatically makes lists for the upcoming stitching example, assuming that the same spacing and bids_format mean we can merge them. You need not read the loop in detail to understand this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "from TPTBox import BIDS_FILE, BIDS_Global_info\n",
    "from TPTBox.core.bids_constants import sequence_splitting_keys\n",
    "\n",
    "print(\"if one of the values of these keys is diffrent, than it is considered a other sequence:\", sequence_splitting_keys)\n",
    "print(\"sub will alway split\")\n",
    "\n",
    "print(\"Lets search for candidate for merging. For this we have to remove the sequ-key from sequence_splitting_keys\")\n",
    "my_splitting_keys = sequence_splitting_keys.copy()\n",
    "my_splitting_keys.remove(\"sequ\")\n",
    "my_splitting_keys.append(\"part\")\n",
    "\n",
    "bgi = BIDS_Global_info(dataset, [\"rawdata\", \"derivative\"], sequence_splitting_keys=my_splitting_keys)\n",
    "stitching_candidate: list[BIDS_FILE] = []\n",
    "epsilon = 0.2\n",
    "for name, subj in bgi.iter_subjects():\n",
    "    print(\"Subject identifier\", name)\n",
    "    q = subj.new_query()\n",
    "    # Filter by some rules\n",
    "    q.flatten()\n",
    "    q.filter_filetype(\"nii.gz\")\n",
    "    q.unflatten()\n",
    "    for fam in q.loop_dict():\n",
    "        print(fam)\n",
    "        for key, file_list in fam.items():\n",
    "            if key == \"mr\":\n",
    "                continue\n",
    "            if len(file_list) == 1:\n",
    "                continue\n",
    "            # This code is only an example, where we group images with the same orientation and zoom, so we know what are potential stitching targets.\n",
    "            # We use _format key as the initial split, so T1w and T2w will not be stiched\n",
    "            matching_group = []\n",
    "            for files in range(len(file_list)):\n",
    "                f1 = file_list[files]\n",
    "                if f1 is None:\n",
    "                    continue\n",
    "                # # We assume the files do not want to get stitched if you have a different one-hundred digits.\n",
    "                idx1 = int(f1.get(\"sequ\", \"0\")) if f1.get(\"sequ\", \"0\").isdigit() else 0  # type: ignore\n",
    "                grid1 = f1.get_grid_info()\n",
    "                if grid1 is None:\n",
    "                    continue\n",
    "                current_group = [f1]  # Start a new group with the current file\n",
    "                for j in range(files + 1, len(file_list)):\n",
    "                    f2 = file_list[j]\n",
    "                    if f2 is None:\n",
    "                        continue\n",
    "                    grid2 = f2.get_grid_info()\n",
    "                    if grid2 is None:\n",
    "                        continue\n",
    "                    idx2 = int(f2.get(\"sequ\", \"0\")) if f1.get(\"sequ\", \"0\").isdigit() else 0  # type: ignore\n",
    "                    if floor(idx1 / 100) != (floor(idx2 / 100)):\n",
    "                        continue\n",
    "                    # Check if orientation matches\n",
    "                    if grid1.orientation == grid2.orientation:\n",
    "                        # Check if zoom is within the tolerance\n",
    "                        zoom_diff = [abs(z1 - z2) for z1, z2 in zip(grid1.zoom, grid2.zoom, strict=False)]\n",
    "                        if all(diff <= epsilon for diff in zoom_diff):\n",
    "                            current_group.append(f2)\n",
    "                            file_list[j] = None  # type: ignore\n",
    "                # Add the group if it has more than one file\n",
    "                if len(current_group) > 1:\n",
    "                    stitching_candidate.append(current_group)\n",
    "for files in stitching_candidate:\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming BIDS data\n",
    "\n",
    "We can create a new BIDS_FILE, by defining changes to a file.\n",
    "\n",
    "Example:\n",
    "\n",
    "/path/to/data/dataset-example/rawdata/sub-0001/ses-12341234/anat/sub-0001_ses-12341234_acq-iso_part-inphase_ct.nii.gz\n",
    "\n",
    "-->\n",
    "\n",
    "/path/to/data/dataset-example/derivatives/sub-0001/ses-12341234/anat/sub-0001_ses-12341234_acq-iso_mod-ct_seg-lung_msk.nii.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import BIDS_FILE\n",
    "\n",
    "f = BIDS_FILE(\n",
    "    \"/path/to/data/dataset-example/rawdata/sub-0001/ses-12341234/anat/sub-0001_ses-12341234_acq-iso_part-inphase_ct.nii.gz\",\n",
    "    dataset=\"/path/to/data/dataset-example/\",\n",
    ")\n",
    "print(f.file[\"nii.gz\"])\n",
    "print(\"-->\")\n",
    "print(\n",
    "    f.get_changed_path(\n",
    "        file_type=\"nii.gz\", bids_format=\"msk\", parent=\"derivatives\", info={\"part\": None, \"mod\": f.bids_format, \"seg\": \"lung\"}\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will validate if you are using defined BIDS keys and throw errors if our validator finds non-standard things. You can turn that off with the 'non_strict_mode' key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how a error message looks like if you use non_strict_mode=False\n",
    "non_strict_mode = True\n",
    "print(\n",
    "    f.get_changed_path(\n",
    "        file_type=\"nii.gz\",\n",
    "        bids_format=\"COSTUME-BIDS-FORMAT\",\n",
    "        parent=\"derivatives\",\n",
    "        info={\"part\": None, \"mod\": f.bids_format, \"seg\": \"lung\"},\n",
    "        non_strict_mode=non_strict_mode,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Stitching  \n",
    "Torax/Fullbody images are often in chunks. We can stich them with the stitching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from TPTBox import to_nii\n",
    "from TPTBox.stitching import stitching\n",
    "\n",
    "derivative_folder = \"rawdata_stiched\"\n",
    "\n",
    "\n",
    "def process_files(files: list[\"BIDS_FILE\"]):\n",
    "    files = sorted(files)  # noqa: PLW2901\n",
    "    sequ: str = (files[0].get(\"sequ\", \"\") + \"-\" if \"sequ\" in files[0].info else \"\") + \"stiched\"  # type: ignore\n",
    "    out_name = files[0].get_changed_path(\"nii.gz\", info={\"sequ\": sequ}, parent=derivative_folder, make_parent=True)\n",
    "    if not out_name.exists():\n",
    "        stitching(files, out=out_name, is_seg=False, is_ct=files[0].bids_format == \"ct\", dtype=to_nii(files[0]).dtype, match_histogram=True)\n",
    "        nii = to_nii(out_name)\n",
    "        nii.apply_crop_(nii.compute_crop())\n",
    "        nii.save(out_name)\n",
    "\n",
    "\n",
    "# Test\n",
    "process_files(stitching_candidate[0])\n",
    "\n",
    "# Execute the loop in parallel using a ProcessPoolExecutor\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    executor.map(process_files, stitching_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a stiched example\n",
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, Visualization_Type, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "files = stitching_candidate[0]\n",
    "files = sorted(files)\n",
    "sequ: str = (files[0].get(\"sequ\", \"\") + \"-\" if \"sequ\" in files[0].info else \"\") + \"stiched\"\n",
    "t = files[0].get_changed_path(\"nii.gz\", info={\"sequ\": sequ}, parent=derivative_folder)\n",
    "a = [Snapshot_Frame(to_nii(i).resample_from_to(t, mode=\"constant\"), sagittal=True) for i in files]\n",
    "b = Snapshot_Frame(t, sagittal=True)\n",
    "\n",
    "create_snapshot(out_img, [*a, b])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Segmentation  \n",
    "\n",
    "Note: by default we do not install Deep-learning stuff.\n",
    "\n",
    "Install:\n",
    "\n",
    "```pip install SPINEPS ruamel.yaml configargparse```\n",
    "\n",
    "trouble shouting: nnunetv2==2.4.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TotalVibeSegmentator\n",
    "\n",
    "https://arxiv.org/abs/2406.00125\n",
    "\n",
    "https://github.com/robert-graf/TotalVibeSegmentator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from TPTBox import BIDS_FILE\n",
    "from TPTBox.segmentation import run_totalvibeseg\n",
    "\n",
    "# run_totalvibeseg\n",
    "# You can also use a string/Path if you want to set the path yourself.\n",
    "### Just making in and output path\n",
    "path_to_dicom_dataset = Path(\"PixelPandemonium\").absolute()\n",
    "target_folder = path_to_dicom_dataset.parent\n",
    "dataset = target_folder / \"dataset-PixelPandemonium\"\n",
    "in_file_dixon = BIDS_FILE(\n",
    "    f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/dixon/sub-111168223_ses-20230128_sequ-501-stiched_acq-ax_part-water_dixon.nii.gz\",\n",
    "    dataset,\n",
    ")\n",
    "out_file_dixon = in_file_dixon.get_changed_path(\n",
    "    \"nii.gz\", \"msk\", parent=\"derivative\", info={\"seg\": \"TotalVibeSegmentator\", \"mod\": in_file_dixon.bids_format}\n",
    ")\n",
    "####\n",
    "run_totalvibeseg(in_file_dixon, out_file_dixon, override=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "a = Snapshot_Frame(\n",
    "    in_file_dixon, segmentation=out_file_dixon, sagittal=True, axial=True, axial_heights=[0.5], ignore_seg_for_centering=True\n",
    ")\n",
    "create_snapshot(out_img, [a])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = BIDS_FILE(\n",
    "    f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/T2w/sub-111168223_ses-20230128_sequ-201-stiched_acq-ax_part-inphase_T2w.nii.gz\",\n",
    "    dataset,\n",
    ")\n",
    "out_file = in_file.get_changed_path(\"nii.gz\", \"msk\", parent=\"derivative\", info={\"seg\": \"TotalVibeSegmentator\", \"mod\": in_file.bids_format})\n",
    "####\n",
    "run_totalvibeseg(in_file, out_file, override=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "a = Snapshot_Frame(in_file, segmentation=out_file, sagittal=True, axial=True, axial_heights=[0.2, 0.4, 0.6], ignore_seg_for_centering=True)\n",
    "create_snapshot(out_img, [a])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spineps\n",
    "\n",
    "Spineps can segment spine images in a instance and semantic mask. Running automatic over a dataset is very opinionated, what to segment. \n",
    "TODO: make a way to manully define output paths\n",
    "\n",
    "https://github.com/Hendrik-code/spineps/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your model is BIDS compliant you can auto run spineps\n",
    "# from TPTBox.segmentation import run_spineps_all\n",
    "# run_spineps_all(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a fitting model:\n",
    "from pathlib import Path\n",
    "\n",
    "from spineps.get_models import modelid2folder_instance, modelid2folder_semantic\n",
    "\n",
    "from TPTBox import BIDS_FILE\n",
    "\n",
    "print(\"Available Semantic Models\", modelid2folder_semantic())\n",
    "print(\"Available Instance Models\", modelid2folder_instance())\n",
    "\n",
    "print(modelid2folder_semantic().keys())\n",
    "print(modelid2folder_instance().keys())\n",
    "\n",
    "### Just making in and output path\n",
    "path_to_dicom_dataset = Path(\"PixelPandemonium\").absolute()\n",
    "target_folder = path_to_dicom_dataset.parent\n",
    "dataset = target_folder / \"dataset-PixelPandemonium\"\n",
    "in_file = BIDS_FILE(\n",
    "    f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/T2w/sub-111168223_ses-20230128_sequ-401-stiched_acq-sag_part-inphase_T2w.nii.gz\",\n",
    "    dataset,\n",
    ")\n",
    "####\n",
    "\n",
    "model_semantic = \"t2w\"\n",
    "model_instance = \"instance\"\n",
    "derivative_name = \"derivative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox.segmentation.spineps import run_spineps_single\n",
    "\n",
    "# With 'ignore_compatibility_issues = True' you can force to run the soft ware\n",
    "out_paths = run_spineps_single(\n",
    "    in_file,\n",
    "    dataset=dataset,\n",
    "    model_semantic=model_semantic,\n",
    "    model_instance=model_instance,\n",
    "    derivative_name=derivative_name,\n",
    "    ignore_compatibility_issues=False,\n",
    "    use_cpu=False,\n",
    "    save_raw=True,\n",
    ")\n",
    "print(out_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "a = Snapshot_Frame(in_file, segmentation=out_paths[\"out_vert\"], centroids=out_paths[\"out_ctd\"], sagittal=True)\n",
    "b = Snapshot_Frame(in_file, segmentation=out_paths[\"out_spine\"], centroids=out_paths[\"out_ctd\"], sagittal=True)\n",
    "create_snapshot(out_img, [a, b])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Point of interest\n",
    "\n",
    "We have a JSON file format that can be rescaled like a nifty between local and global spaces.\n",
    "\n",
    "The file is human-readable. Note: the POI file and numpy start counting with 0, while ITKSnap starts counting with 1. You have to subtract one in ITKSnap.\n",
    "\n",
    "Loading a file you get a \"POI\" object. Every entry has two levels. They can become liberal. For Vertebra, it is `Vertebra-ID,` `Point number.`\n",
    "\n",
    "https://doi.org/10.3389/fbioe.2022.862804 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import NII, POI\n",
    "\n",
    "nii_path = in_file\n",
    "nii = NII.load(nii_path, False)\n",
    "# Making a POI object in the same space as an image\n",
    "poi_obj: POI = nii.make_empty_POI()\n",
    "print(f\"{nii =}\")\n",
    "print(f\"{poi_obj=}\")\n",
    "# You can set, read poi Objects like you would access a 2D dictionary\n",
    "poi_obj[19, 50] = (10, 20, 30)\n",
    "print(\"Before rescaling\", poi_obj[19, 50])\n",
    "# We can use rescale, reorient, resample_from_to like in a nii\n",
    "poi_obj = poi_obj.rescale((0.5, 0.5, 0.5))\n",
    "print(\"After rescaling\", poi_obj[19, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common way to fill a POI object is by computing them from a Segmentation\n",
    "\n",
    "Lets compute the center of mass of a instance segmentation. Like from SPINEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import Location, calc_centroids\n",
    "\n",
    "nii_instance_path = in_file  # MRI File\n",
    "nii_instance_nii = NII.load(out_paths[\"out_vert\"], seg=True)  # Instance mask\n",
    "nii_instance_nii[nii_instance_nii > 100] = 0  # Only numbers below 100 are Vertebras\n",
    "\n",
    "# You have to set one of the two keys. The other is coming from the numbers in the segmentation\n",
    "print(\"First stage value (first_stage)\", nii_instance_nii.unique())\n",
    "print(\"Second stage value (second_stage)\", Location.Vertebra_Full.value, \"\\n\")\n",
    "poi = calc_centroids(nii_instance_nii, second_stage=Location.Vertebra_Full.value)\n",
    "\n",
    "print(poi, \"num-points:\", len(poi), \"\\n\")\n",
    "print(\"keys: \", poi.keys())\n",
    "print(f\"example point {poi[nii_instance_nii.unique()[0],Location.Vertebra_Full.value] =}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, Visualization_Type, create_snapshot\n",
    "\n",
    "out_img = Path(\"out.jpg\")\n",
    "a = Snapshot_Frame(\n",
    "    in_file,\n",
    "    segmentation=nii_instance_nii,\n",
    "    centroids=poi,\n",
    "    sagittal=True,\n",
    "    only_mask_area=True,\n",
    "    visualization_type=Visualization_Type.Maximum_Intensity,\n",
    ")\n",
    "create_snapshot(out_img, [a])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Vertebra segmentation, we have a rule-based pipeline to generate points.\n",
    "\n",
    "You need the instance and subregion mask from spineps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from TPTBox import Location, Vertebra_Instance, calc_poi_from_subreg_vert\n",
    "\n",
    "nii_instance_path = out_paths[\"out_vert\"]\n",
    "nii_semantic_path = out_paths[\"out_spine\"]\n",
    "\n",
    "# We can compute the direction of the Vertebra\n",
    "subreg_ids = [\n",
    "    Location.Vertebra_Corpus,\n",
    "    Location.Vertebra_Direction_Inferior,\n",
    "    Location.Vertebra_Direction_Right,\n",
    "    Location.Vertebra_Direction_Posterior,\n",
    "]\n",
    "poi_fixed = calc_poi_from_subreg_vert(nii_instance_path, nii_semantic_path, subreg_id=subreg_ids)\n",
    "# Note: we may need to compute additional points that than are also in the poi file.\n",
    "poi_fixed.extract_subregion_(*subreg_ids)\n",
    "\n",
    "\n",
    "### PRINTING ####\n",
    "print(poi_fixed, \"\\n\")\n",
    "\n",
    "print(\"L1 Corpus\", poi_fixed[Vertebra_Instance.L1, Location.Vertebra_Corpus], \"\\n\")\n",
    "\n",
    "v = np.array(poi_fixed[Vertebra_Instance.L1, Location.Vertebra_Direction_Inferior]) - np.array(\n",
    "    poi_fixed[Vertebra_Instance.L1, Location.Vertebra_Corpus]\n",
    ")\n",
    "v /= np.sqrt((v**2).sum())\n",
    "print(\"normal down drection\", v, poi_fixed.orientation, \"\\n\")\n",
    "poi_fixed_slp = poi_fixed.reorient((\"S\", \"L\", \"P\"))\n",
    "v = np.array(poi_fixed_slp[Vertebra_Instance.L1, Location.Vertebra_Direction_Inferior]) - np.array(\n",
    "    poi_fixed_slp[Vertebra_Instance.L1, Location.Vertebra_Corpus]\n",
    ")\n",
    "v /= np.sqrt((v**2).sum())\n",
    "print(\"normal down drection after reorientation\", v, poi_fixed_slp.orientation, \"\\n\")\n",
    "##Not this is normalized to image space. If you want the direction in global space, resample to (1,1,1) mm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, Visualization_Type, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
<<<<<<< HEAD
    "a = Snapshot_Frame(in_file, segmentation=out_paths[\"out_vert\"], centroids=poi_fixed, coronal=True, sagittal=True)\n",
    "b = Snapshot_Frame(in_file, segmentation=out_paths[\"out_spine\"], centroids=poi_fixed, sagittal=True)\n",
    "create_snapshot(out_img, [a, b])\n",
=======
    "a = Snapshot_Frame(in_file, segmentation=out_paths[\"out_vert\"],centroids=poi_fixed,coronal=True, sagittal=True)\n",
    "b = Snapshot_Frame(in_file, segmentation=out_paths[\"out_spine\"],centroids=poi_fixed, sagittal=True)\n",
    "create_snapshot(out_img, [a,b])\n",
>>>>>>> c146e97 (Dicom extreaction and deformable reg (#59))
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the points described in \n",
    "\n",
    "\"Validation of a Patient-Specific Musculoskeletal Model for Lumbar Load Estimation Generated by an Automated Pipeline From Whole Body CT\"\n",
    "\n",
    "https://doi.org/10.3389/fbioe.2022.862804 \n",
    "\n",
    "![Special POIs](https://www.frontiersin.org/files/Articles/862804/fbioe-10-862804-HTML/image_m/fbioe-10-862804-g002.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "new_pois = [\n",
    "    Location.Vertebra_Corpus,\n",
    "    Location.Additional_Vertebral_Body_Middle_Inferior_Median,\n",
    "    Location.Additional_Vertebral_Body_Middle_Superior_Median,\n",
    "    Location.Additional_Vertebral_Body_Posterior_Central_Median,\n",
    "    Location.Additional_Vertebral_Body_Anterior_Central_Median,\n",
    "    Location.Ligament_Attachment_Point_Anterior_Longitudinal_Superior_Median,\n",
    "    Location.Ligament_Attachment_Point_Posterior_Longitudinal_Superior_Median,\n",
    "    Location.Ligament_Attachment_Point_Anterior_Longitudinal_Inferior_Median,\n",
    "    Location.Ligament_Attachment_Point_Posterior_Longitudinal_Inferior_Median,\n",
    "]\n",
    "\n",
    "poi_fixed = calc_poi_from_subreg_vert(\n",
    "    nii_instance_path, nii_semantic_path, subreg_id=[Location.Vertebra_Direction_Inferior, *new_pois]\n",
    ").extract_subregion(*new_pois)\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(in_file, segmentation=out_paths[\"out_vert\"], centroids=poi_fixed, sagittal=True)\n",
    "b = Snapshot_Frame(in_file, segmentation=out_paths[\"out_spine\"], centroids=poi_fixed, sagittal=True)\n",
    "create_snapshot(out_img, [a, b])\n",
    "Image(filename=out_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Point Registration\n",
    "\n",
    "We can use two POI objects for point registration. Only points that exist in both POIs are considered. \n",
    "\n",
    "\n",
    "We recommend spine registration at least two points per Vertebra to prevent rotation around the spine. See https://doi.org/10.1186/s41747-023-00385-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import to_nii\n",
    "\n",
    "### MAKE an artificial example ###\n",
    "dataset = target_folder / \"dataset-PixelPandemonium\"\n",
    "nii_instance = to_nii(out_paths[\"out_vert\"], True)\n",
    "nii_semantic = to_nii(out_paths[\"out_spine\"], True)\n",
    "fixed_image = to_nii(\n",
    "    f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/T2w/sub-111168223_ses-20230128_sequ-401-stiched_acq-sag_part-inphase_T2w.nii.gz\"\n",
    ")\n",
    "\n",
    "crop1 = (slice(None), slice(None), slice(0, 2000))\n",
    "crop2 = (slice(None), slice(None), slice(1500, None))\n",
    "\n",
    "nii_instance1 = nii_instance.copy().apply_crop(crop1)\n",
    "nii_semantic1 = nii_semantic.copy().apply_crop(crop1)\n",
    "fixed_image1 = fixed_image.copy().apply_crop(crop1)\n",
    "\n",
    "nii_instance2 = nii_instance.copy().apply_crop(crop2)\n",
    "nii_semantic2 = nii_semantic.copy().apply_crop(crop2)\n",
    "image2 = fixed_image.copy().apply_crop(crop2)\n",
    "\n",
    "nii_instance2.origin = (nii_instance2.origin[0] + 1, nii_instance2.origin[1] - 3, nii_instance2.origin[2] - 190)\n",
    "nii_semantic2.origin = nii_instance2.origin\n",
    "image2.origin = nii_instance2.origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import NII, Location, Vertebra_Instance, calc_poi_from_subreg_vert\n",
    "from TPTBox.registration import Point_Registration, ridged_points_from_poi\n",
    "from TPTBox.segmentation.TotalVibeSeg import extract_vertebra_bodies_from_totalVibe\n",
    "\n",
    "# Example registration two sagittal images, like for compensating movement between scans.\n",
    "poi_fixed = calc_poi_from_subreg_vert(nii_instance1, nii_semantic1, subreg_id=[Location.Vertebra_Corpus, Location.Spinosus_Process]).round(\n",
    "    2\n",
    ")\n",
    "poi_moving = calc_poi_from_subreg_vert(nii_instance2, nii_semantic2, subreg_id=[Location.Vertebra_Corpus, Location.Spinosus_Process]).round(\n",
    "    2\n",
    ")\n",
    "\n",
    "registration_object: Point_Registration = ridged_points_from_poi(poi_fixed, poi_moving, c_val=0)\n",
    "\n",
    "# Move image\n",
    "moved_nii = registration_object.transform_nii(image2, output_space=fixed_image)\n",
    "print(moved_nii, \"\\n\", image2, \"\\n\")\n",
    "# Move poi\n",
    "moved_poi = registration_object.transform_poi(poi_moving).round(1)\n",
    "print(moved_poi, \"\\n\", poi_moving, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "poi_all = moved_poi.resample_from_to(moved_nii).join_left(poi_fixed.resample_from_to(moved_nii))\n",
    "x = Snapshot_Frame(image2.resample_from_to(moved_nii, mode=\"constant\"), centroids=poi_moving.resample_from_to(moved_nii), sagittal=True)\n",
    "a = Snapshot_Frame(\n",
    "    fixed_image1.resample_from_to(moved_nii, mode=\"constant\"), centroids=poi_fixed.resample_from_to(moved_nii), sagittal=True\n",
    ")\n",
    "b = Snapshot_Frame(moved_nii, centroids=moved_poi.resample_from_to(moved_nii), sagittal=True)\n",
    "c = Snapshot_Frame(fixed_image, centroids=poi_all, sagittal=True)\n",
    "create_snapshot(out_img, [x, a, b, c])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import NII, Location, Vertebra_Instance, calc_poi_from_subreg_vert\n",
    "from TPTBox.registration import Point_Registration, ridged_points_from_poi\n",
    "from TPTBox.segmentation.TotalVibeSeg import extract_vertebra_bodies_from_totalVibe\n",
    "\n",
    "# Example registration axial and sagittal with points.\n",
    "# T2w axial points are computed from the total vibe segment.\n",
    "dataset = target_folder / \"dataset-PixelPandemonium\"\n",
    "\n",
    "nii_instance_path2 = out_paths[\"out_vert\"]\n",
    "nii_semantic_path2 = out_paths[\"out_spine\"]\n",
    "# We recommend if you registrade spineps points too use at least two points.\n",
    "poi_fixed = calc_poi_from_subreg_vert(nii_instance_path, nii_semantic_path, subreg_id=[Location.Vertebra_Corpus]).round(1)\n",
    "fixed_image = BIDS_FILE(\n",
    "    f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/T2w/sub-111168223_ses-20230128_sequ-401-stiched_acq-sag_part-inphase_T2w.nii.gz\",\n",
    "    dataset,\n",
    ")\n",
    "\n",
    "moving_file = BIDS_FILE(\n",
    "    f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/dixon/sub-111168223_ses-20230128_sequ-501-stiched_acq-ax_part-water_dixon.nii.gz\",\n",
    "    dataset,\n",
    ")\n",
    "out_file = moving_file.get_changed_path(\n",
    "    \"nii.gz\", \"msk\", parent=\"derivative\", info={\"seg\": \"TotalVibeSegmentator\", \"mod\": moving_file.bids_format}\n",
    ")\n",
    "moving_image = to_nii(moving_file)\n",
    "out_file = to_nii(out_file, True)\n",
    "\n",
    "#### Let's move the image, so we see better that the registration is working.\n",
    "moving_image.origin = (moving_image.origin[0], moving_image.origin[1], moving_image.origin[2] + 175)\n",
    "out_file.origin = moving_image.origin\n",
    "####\n",
    "\n",
    "# Ensure that we count the same as in the T2w sagittal\n",
    "num_thoracic_verts = 12\n",
    "if Vertebra_Instance.T13.value in poi_fixed.keys_region():\n",
    "    num_thoracic_verts = 13\n",
    "if Vertebra_Instance.T12.value not in poi_fixed.keys_region():\n",
    "    num_thoracic_verts = 11\n",
    "num_lumbar_verts = 5\n",
    "if Vertebra_Instance.L6.value in poi_fixed.keys_region():\n",
    "    num_lumbar_verts = 6\n",
    "if Vertebra_Instance.L5.value not in poi_fixed.keys_region():\n",
    "    num_lumbar_verts = 4\n",
    "# Note: this function currently assumes that we see the sacrum in the image.\n",
    "nii, poi_moving = extract_vertebra_bodies_from_totalVibe(out_file, num_lumbar_verts=num_lumbar_verts, num_thoracic_verts=num_thoracic_verts)\n",
    "\n",
    "registration_object: Point_Registration = ridged_points_from_poi(poi_fixed, poi_moving, c_val=0)\n",
    "\n",
    "# Move image\n",
    "moved_nii = registration_object.transform_nii(moving_image)\n",
    "print(moved_nii, \"\\n\", moving_image, \"\\n\")\n",
    "# Move poi\n",
    "moved_poi = registration_object.transform_poi(poi_moving).round(1)\n",
    "print(moved_poi, \"\\n\", poi_moving, \"\\n\")\n",
    "print(moving_image.shape, moving_image.affine.reshape((-1,)).tolist(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(moving_image, segmentation=nii, centroids=poi_moving, sagittal=True)\n",
    "create_snapshot(out_img, [a])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(moving_image.resample_from_to(moved_nii, mode=\"constant\"), centroids=poi_fixed, sagittal=True)\n",
    "b = Snapshot_Frame(moved_nii, centroids=poi_fixed, sagittal=True)\n",
    "c = Snapshot_Frame(fixed_image, centroids=poi_fixed, sagittal=True)\n",
    "create_snapshot(out_img, [a, b, c])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Deformable registration for different breath holds compensation\n",
    "\n",
    "With our point pre-registration, we can use deformable registration to remove breathing.\n",
    "\n",
    "(Our example did not suffer from breathing movement, so we used similar images from ammos22)\n",
    "\n",
    "This example needs Deepali installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from TPTBox import to_nii\n",
    "from TPTBox.registration.deformable import Deformable_Registration\n",
    "\n",
    "moving_nii = to_nii(Path(\"PixelPandemonium/PixelPandemonium/mr_0.nii.gz\").absolute(), False)\n",
    "fixed_nii = to_nii(Path(\"PixelPandemonium/PixelPandemonium/mr_1.nii.gz\").absolute(), False)\n",
    "\n",
    "\n",
    "reg = Deformable_Registration(fixed_nii, moving_nii, normalize=\"MRI\", device=\"cuda\")\n",
    "\n",
    "moved_nii = reg.transform_nii(moving_nii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(moving_nii.resample_from_to(moved_nii, mode=\"constant\"), sagittal=False, axial=True, axial_heights=[0.5], mode=\"MRI\")\n",
    "b = Snapshot_Frame(moved_nii, sagittal=False, axial=True, axial_heights=[0.5], mode=\"MRI\")\n",
    "c = Snapshot_Frame(fixed_nii.resample_from_to(moved_nii, mode=\"constant\"), sagittal=False, axial=True, axial_heights=[0.5], mode=\"MRI\")\n",
    "create_snapshot(out_img, [a, b, c])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(moving_nii.resample_from_to(moved_nii, mode=\"constant\"), sagittal=True, mode=\"MRI\")\n",
    "b = Snapshot_Frame(moved_nii, sagittal=True, mode=\"MRI\")\n",
    "c = Snapshot_Frame(fixed_nii.resample_from_to(moved_nii, mode=\"constant\"), sagittal=True, mode=\"MRI\")\n",
    "create_snapshot(out_img, [a, b, c])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
