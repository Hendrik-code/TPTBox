{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Overview\n",
    "\n",
    "This tutorial will give you an overview how to get from a DICOM dump to a processed Dataset with segmentations.\n",
    "\n",
    "abbreviations:\n",
    "POI: Point of interest\n",
    "\n",
    "Steps:\n",
    "\n",
    "(1) Dicom export to BIDS dataset\n",
    "\n",
    "(2) Stitching\n",
    "\n",
    "(3) Segmentation TotalVibeSegmentator, Spineps ...\n",
    "\n",
    "(4) Points of Interest (POI) \n",
    "\n",
    "(5) Point registation\n",
    "\n",
    "(2) ~~Inter-scan image registrationâ€‹.~~\n",
    "\n",
    "(X) ~~Rigide Movement correction with automatic Spine POIs~~\n",
    "\n",
    "(x) ~~Deformable Movement~~\n",
    "\n",
    "(x) ~~Stitching with rigid movement compensation. (From 2.1)~~\n",
    "\n",
    "(x) ~~Stitching with deformable movement compensation. (From 2.2)~~\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(5) ~~MR Deformable Registration (From 2.1,2.2)~~\n",
    "\n",
    "(6) ~~Water Fat Swap detection in VIBE and MEVIBE~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dicom export to BIDS dataset\n",
    "\n",
    "Short overview:\n",
    "\n",
    "A BIDS dataset is a file naming convection.\n",
    "\n",
    "The following rules should be known and weakly enforced:\n",
    "\n",
    "- A dataset folder should start with 'dataset-{YOUR-NAME}'\n",
    "- The next level folder are:\n",
    "  - rawdata: for all imaging data.\n",
    "  - derivative: for all generated data, like segmentation.\n",
    "A file should look like:\n",
    "\n",
    "sub-{Subject name}_ses-{Session}_{key}-{value}*_{format}.{filetype}\n",
    "- Subject name: Unique identifier \n",
    "- Session: Session id. Optional if there is only one session\n",
    "- Any number of key-values. Keys are unique. The defined keys are here: https://bids-specification.readthedocs.io/en/stable/appendices/entities.html . Our tool enforces a certain order. See tutorial_BIDS_files.ipynb\n",
    "- format: type of acquisition like ct, T2w, VIBE, MPRage\n",
    "Do not use '_' in any key or values. \n",
    "\n",
    "See https://bids-specification.readthedocs.io/en/stable/ for detailed description what BIDS ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox.core.bids_files import entities_keys, formats\n",
    "\n",
    "print('Known formats:\\n','\\n'.join(formats))\n",
    "print()\n",
    "print()\n",
    "print(\"Order of keys we enforce:\\n\", '\\n'.join(entities_keys.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This function extracts a dicom folder to a BIDS-like Niffty folder.\n",
    "\n",
    "The names are created like this: DICOM:Key is given dicom key\n",
    "      \n",
    "`dataset-{NAME}/rawdate/sub-{DICOM:PatientID}/ses-{DICOM:StudyDate}/{format}/sub-{DICOM:PatientID}_ses-{DICOM:StudyDate}_sequ-{DICOM:SeriesNumber}_acq-{sag|ax|cor|iso}_{format}.nii.gz`\n",
    "\n",
    "\n",
    "and a .json, where the and DICOM-Keys are saved.\n",
    "\n",
    "To get {format} we use string matching and the dicom \"SeriesDescription\" key. As this is a free text this will not always work. Than we default to \"mr\" and you have to manually rename them.\n",
    "\n",
    "\n",
    "For very large dataset you can use make_subject_chunks = n [int]. Than we put a additional folder with the first n letters between rawdata and the sub- folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "path_to_dicom_dataset = \"TODO\" # TODO\n",
    "dataset_name = 'example-name' # TODO\n",
    "\n",
    "path_to_dicom_dataset = \"/media/data/robert/datasets/dicom_example/Nispel_MRT/\" # TODO Remove\n",
    "dataset_name = 'Nispel_MRT' # TODO Remove\n",
    "target_folder = Path(path_to_dicom_dataset).parent\n",
    "dataset = target_folder / f\"dataset-{dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from TPTBox.core.dicom.dicom_extract import extract_dicom_folder\n",
    "\n",
    "extract_dicom_folder(Path(path_to_dicom_dataset), dataset,use_session=True,n_cpu=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tool that automat scans Bids folders an creates a grouped dictionary, where you can pick out the relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import BIDS_FILE, BIDS_Global_info\n",
    "from TPTBox.core.bids_constants import sequence_splitting_keys\n",
    "\n",
    "print(\"if one of the values of these keys is diffrent, than it is considered a other sequence:\", sequence_splitting_keys)\n",
    "print(\"sub will alway split\")\n",
    "\n",
    "print(\"Lets search for candidate for merging. For this we have to remove the sequ-key from sequence_splitting_keys\")\n",
    "my_splitting_keys = sequence_splitting_keys.copy()\n",
    "my_splitting_keys.remove(\"sequ\")\n",
    "my_splitting_keys.append(\"part\")\n",
    "\n",
    "bgi = BIDS_Global_info(dataset,[\"rawdata\",\"derivative\"],sequence_splitting_keys=my_splitting_keys)\n",
    "stitching_candidate:list[BIDS_FILE] = []\n",
    "epsilon = 0.2\n",
    "for name, subj in bgi.iter_subjects():\n",
    "    print('Subject identifier',name)\n",
    "    q = subj.new_query()\n",
    "    #Filter by some rules\n",
    "    q.flatten()\n",
    "    q.filter_filetype('nii.gz')\n",
    "    q.unflatten()\n",
    "    for fam in q.loop_dict():\n",
    "        print(fam)\n",
    "        for key, file_list in fam.items():\n",
    "            if key == \"mr\":\n",
    "                continue\n",
    "            if len(file_list) == 1:\n",
    "                continue\n",
    "            # This code is only an example, where we group images with the same orientation and zoom, so we know what are potential stitching targets.\n",
    "            # We use _format key as the initial split, so T1w and T2w will not be stiched\n",
    "            matching_group = []\n",
    "            for files in range(len(file_list)):\n",
    "                f1 = file_list[files]\n",
    "                if f1 is None:\n",
    "                    continue\n",
    "                grid1 = f1.get_grid_info()\n",
    "                if grid1 is None:\n",
    "                    continue\n",
    "                current_group = [f1]  # Start a new group with the current file\n",
    "                for j in range(files + 1, len(file_list)):\n",
    "                    f2 = file_list[j]\n",
    "                    if f2 is None:\n",
    "                        continue\n",
    "                    grid2 = f2.get_grid_info()\n",
    "                    if grid2 is None:\n",
    "                        continue\n",
    "                    # Check if orientation matches\n",
    "                    if grid1.orientation == grid2.orientation:\n",
    "                        # Check if zoom is within the tolerance\n",
    "                        zoom_diff = [abs(z1 - z2) for z1, z2 in zip(grid1.zoom, grid2.zoom,strict=False)]\n",
    "                        if all(diff <= epsilon for diff in zoom_diff):\n",
    "                            current_group.append(f2)\n",
    "                            file_list[j] = None # type: ignore\n",
    "                # Add the group if it has more than one file\n",
    "                if len(current_group) > 1:\n",
    "                    stitching_candidate.append(current_group)\n",
    "for files in stitching_candidate:\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Stitching  \n",
    "Torax/Fullbody images are often in chunks. We can stich them with the stitching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from TPTBox import to_nii\n",
    "from TPTBox.stitching import stitching\n",
    "\n",
    "derivative_folder = \"rawdata_stiched\"\n",
    "\n",
    "def process_files(files):\n",
    "    files = sorted(files)  # noqa: PLW2901\n",
    "    sequ: str = (files[0].get(\"sequ\", \"\") + \"-\" if \"sequ\" in files[0].info else \"\") + \"stiched\"  # type: ignore\n",
    "    out_name = files[0].get_changed_path(\"nii.gz\", info={\"sequ\": sequ}, parent=derivative_folder)\n",
    "    if not out_name.exists():\n",
    "        stitching(files, out=out_name, is_seg=False, is_ct=files[0].bids_format == \"ct\", dtype=to_nii(files[0]).dtype)\n",
    "        nii = to_nii(out_name)\n",
    "        nii.apply_crop_(nii.compute_crop())\n",
    "        nii.save(out_name)\n",
    "# Test\n",
    "process_files(stitching_candidate[0])\n",
    "# Execute the loop in parallel using a ProcessPoolExecutor\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    executor.map(process_files, stitching_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Segmentation  \n",
    "\n",
    "Note: by default we do not install Deep-learning stuff.\n",
    "\n",
    "Install:\n",
    "\n",
    "```pip install SPINEPS ruamel.yaml configargparse```\n",
    "\n",
    "trouble shouting: nnunetv2==2.4.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TotalVibeSegmentator\n",
    "\n",
    "https://arxiv.org/abs/2406.00125\n",
    "\n",
    "https://github.com/robert-graf/TotalVibeSegmentator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import BIDS_FILE\n",
    "from TPTBox.segmentation import run_totalvibeseg\n",
    "\n",
    "# run_totalvibeseg\n",
    "# You can alos use a string/Path if you want to set the path yourself.\n",
    "dataset = \"/media/data/robert/datasets/dicom_example/dataset-VR-DICOM2/\"\n",
    "in_file = BIDS_FILE(f\"{dataset}/derivative_stiched/sub-111168222/T2w/sub-111168222_sequ-301-stiched_acq-ax_part-water_T2w.nii.gz\",dataset)\n",
    "out_file = in_file.get_changed_path(\"nii.gz\",\"msk\",parent=\"derivative\",info={\"seg\":\"TotalVibeSegmentator\",\"mod\":in_file.bids_format})\n",
    "run_totalvibeseg(in_file,out_file,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spineps\n",
    "\n",
    "Spineps can segment spine images in a instance and semantic mask. Running automatic over a dataset is very opinionated, what to segment. \n",
    "TODO: make a way to manully define output paths\n",
    "\n",
    "https://github.com/Hendrik-code/spineps/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your model is BIDS compliant you can auto run spineps\n",
    "from TPTBox.segmentation import run_spineps_all\n",
    "run_spineps_all(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a fitting model:\n",
    "from spineps.models import modelid2folder_instance, modelid2folder_semantic\n",
    "\n",
    "print('Available Semantic Models',modelid2folder_semantic())\n",
    "print('Available Instance Models',modelid2folder_instance())\n",
    "\n",
    "print(modelid2folder_semantic().keys())\n",
    "print(modelid2folder_instance().keys())\n",
    "dataset = \"/media/data/robert/datasets/dicom_example/dataset-VR-DICOM2\"\n",
    "in_file = f\"{dataset}/rawdata/sub-0003106805/ses-20240905/T2w//sub-0003106805_ses-20240905_sequ-303_acq-sag_part-inphase_T2w.nii.gz\"\n",
    "\n",
    "model_semantic = \"t2w\"\n",
    "model_instance = \"instance\"\n",
    "derivative_name = \"derivative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import to_nii\n",
    "to_nii(f\"{dataset}/rawdata/sub-0003106805/ses-20240905/T2w//sub-0003106805_ses-20240905_sequ-303_acq-sag_part-inphase_T2w.nii.gz\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox.segmentation.spineps import run_spineps_single\n",
    "\n",
    "#With 'ignore_compatibility_issues = True' you can force to rund\n",
    "out_paths = run_spineps_single(\n",
    "    in_file,\n",
    "    dataset=dataset,\n",
    "    model_semantic=model_semantic,\n",
    "    model_instance=model_instance,\n",
    "    derivative_name=derivative_name,\n",
    "    ignore_compatibility_issues=False,use_cpu=True,)\n",
    "print(out_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Point of intresst\n",
    "\n",
    "We have a json file format, that can be rescaled like a niffty between loacal spaceses and to global space.\n",
    "\n",
    "The file is human reable. Not the file and numpy start conting with 0, while ITKSnap start conting with 1. You have to suptract one in ITKSnap.\n",
    "\n",
    "Loading a file you get a \"POI\" object. Every entry has two levels. They can bechooen abetrally. For Vertebra it is `Vertebra-ID`, `Point number`.\n",
    "\n",
    "https://doi.org/10.3389/fbioe.2022.862804 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import POI, NII\n",
    "\n",
    "nii_path = \"/DATA/NAS/datasets_processed/NAKO/dataset-nako/rawdata_stitched/100/100000/T2w/sub-100000_sequ-stitched_acq-sag_T2w.nii.gz\"\n",
    "nii = NII.load(nii_path,False)\n",
    "# Making a POI object in the same space as an image\n",
    "poi_obj:POI = nii.get_empty_POI()\n",
    "print(nii)\n",
    "print(poi_obj)\n",
    "# You can set,read poi Objects like you would accses a 2D Dictonary\n",
    "poi_obj[19,50] = (10,20,30)\n",
    "print('Before rescaling',poi_obj[19,50])\n",
    "# We can use rescale, reorient, resample_from_to like in a nii\n",
    "poi_obj = poi_obj.rescale((0.5,.5,.5))\n",
    "print('After rescaling',poi_obj[19,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common way to fill a POI object is by computing them from a Segmentation\n",
    "\n",
    "Lets compute the center of mass of a instance segmentation. Like from SPINEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import calc_centroids,Location\n",
    "\n",
    "nii_instance_path = \"/DATA/NAS/datasets_processed/NAKO/dataset-nako/derivatives_spine_inference_combination162_148/100/100000/T2w/sub-100000_sequ-stitched_acq-sag_mod-T2w_seg-vert_msk.nii.gz\"\n",
    "nii_instance_nii = NII.load(nii_instance_path, seg=True)\n",
    "nii_instance_nii[nii_instance_nii>100] = 0 # Only numbers below 100 are Vertebras\n",
    "\n",
    "# You have to set one of the two keys. The other is coming from the numbers in the segmentation\n",
    "print('First stage value (first_stage)',nii_instance_nii.unique())\n",
    "print('Second stage value (second_stage)',Location.Vertebra_Full.value,'\\n')\n",
    "poi = calc_centroids(nii_instance_nii,second_stage=Location.Vertebra_Full.value)\n",
    "\n",
    "print(poi,'\\n')\n",
    "\n",
    "print(f\"example point {poi[nii_instance_nii.unique()[0],Location.Vertebra_Full.value] =}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Vertebra segmentation we have a rule based pipline to generate points.\n",
    "\n",
    "You need the instance and subregion mask from spineps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import calc_poi_from_subreg_vert,Location,Vertebra_Instance\n",
    "import numpy as np\n",
    "nii_instance_path = \"/DATA/NAS/datasets_processed/NAKO/dataset-nako/derivatives_spine_inference_combination162_148/100/100000/T2w/sub-100000_sequ-stitched_acq-sag_mod-T2w_seg-vert_msk.nii.gz\"\n",
    "nii_semantic_path = \"/DATA/NAS/datasets_processed/NAKO/dataset-nako/derivatives_spine_inference_combination162_148/100/100000/T2w/sub-100000_sequ-stitched_acq-sag_mod-T2w_seg-spine_msk.nii.gz\"\n",
    "\n",
    "\n",
    "poi_fixed = calc_poi_from_subreg_vert(nii_instance_path,nii_semantic_path,subreg_id=[Location.Vertebra_Corpus,Location.Vertebra_Direction_Inferior,Location.Vertebra_Direction_Right,Location.Vertebra_Direction_Posterior])\n",
    "\n",
    "print(poi_fixed,\"\\n\")\n",
    "\n",
    "print(\"L1 Corpus\",poi_fixed[Vertebra_Instance.L1,Location.Vertebra_Corpus],\"\\n\")\n",
    "\n",
    "v = np.array(poi_fixed[Vertebra_Instance.L1,Location.Vertebra_Direction_Inferior]) - np.array(poi_fixed[Vertebra_Instance.L1,Location.Vertebra_Corpus])\n",
    "v /= np.sqrt((v**2).sum())\n",
    "print(\"normal down drection\",v,poi_fixed.orientation,\"\\n\")\n",
    "poi_fixed.reorient_((\"S\",\"L\",\"P\"))\n",
    "v = np.array(poi_fixed[Vertebra_Instance.L1,Location.Vertebra_Direction_Inferior]) - np.array(poi_fixed[Vertebra_Instance.L1,Location.Vertebra_Corpus])\n",
    "v /= np.sqrt((v**2).sum())\n",
    "print(\"normal down drection after reorientation\",v,poi_fixed.orientation,\"\\n\")\n",
    "#Not this is normalizet to image space. If you want the dirction in global space resample to (1,1,1) mm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Point registation\n",
    "\n",
    "We can use two POI object to do ridghed point registation. Only point are considert that exist in both POIs. \n",
    "\n",
    "\n",
    "We recomed for spine registation atleast two points per Vertebra, to prevent roation around the spine. See https://doi.org/10.1186/s41747-023-00385-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox.registration import ridged_points_from_poi,Point_Registration\n",
    "from TPTBox import calc_poi_from_subreg_vert,Location,NII\n",
    "\n",
    "nii_instance_path = \"/DATA/NAS/datasets_processed/NAKO/dataset-nako/derivatives_spine_inference_combination162_148/100/100000/T2w/sub-100000_sequ-stitched_acq-sag_mod-T2w_seg-vert_msk.nii.gz\"\n",
    "nii_semantic_path = \"/DATA/NAS/datasets_processed/NAKO/dataset-nako/derivatives_spine_inference_combination162_148/100/100000/T2w/sub-100000_sequ-stitched_acq-sag_mod-T2w_seg-spine_msk.nii.gz\"\n",
    "poi_fixed = calc_poi_from_subreg_vert(nii_instance_path,nii_semantic_path,subreg_id=[Location.Vertebra_Corpus,Location.Spinosus_Process]).round(1)\n",
    "\n",
    "\n",
    "nii_instance_path2 = \"/DATA/NAS/datasets_processed/NAKO/dataset-nako/derivatives_spine_inference_combination162_148/100/100000/T2w/sub-100000_sequ-stitched_acq-sag_mod-T2w_seg-vert_msk.nii.gz\"\n",
    "nii_semantic_path2 = \"/DATA/NAS/datasets_processed/NAKO/dataset-nako/derivatives_spine_inference_combination162_148/100/100000/T2w/sub-100000_sequ-stitched_acq-sag_mod-T2w_seg-spine_msk.nii.gz\"\n",
    "poi_moving = calc_poi_from_subreg_vert(nii_instance_path,nii_semantic_path,subreg_id=[Location.Vertebra_Corpus,Location.Spinosus_Process]).round(1)\n",
    "moving_image = \"/DATA/NAS/datasets_processed/NAKO/dataset-nako/derivatives_spine_inference_combination162_148/100/100000/T2w/sub-100000_sequ-stitched_acq-sag_mod-T2w_seg-vert_msk.nii.gz\"\n",
    "\n",
    "\n",
    "registation_object:Point_Registration = ridged_points_from_poi(poi_fixed,poi_moving)\n",
    "\n",
    "# Move image\n",
    "moving_nii = NII.load(moving_image,False)\n",
    "moved_nii = registation_object.transform_nii(moving_nii)\n",
    "print(moved_nii,'\\n',moving_nii,'\\n')\n",
    "# Move poi\n",
    "moved_poi = registation_object.transform_poi(poi_moving).round(1)\n",
    "print(moved_poi,'\\n',poi_moving,'\\n')\n",
    "# Move image by updating the affine, but not resampe the image\n",
    "moving_nii_affine_only = registation_object.transform_nii_affine_only(moving_nii)\n",
    "print(moving_nii_affine_only.shape,moving_nii_affine_only.affine.reshape((-1,)).tolist())\n",
    "print(moving_nii.shape,moving_nii.affine.reshape((-1,)).tolist(),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
