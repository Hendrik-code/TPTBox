{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install TPTBox ruamel.yaml configargparse spineps hf-deepali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This tutorial will give you an overview how to get from a DICOM dump to a processed Dataset with segmentations.\n",
    "\n",
    "abbreviations:\n",
    "POI: Point of interest\n",
    "\n",
    "Steps:\n",
    "\n",
    "(1) Dicom export to BIDS dataset\n",
    "\n",
    "(2) Stitching\n",
    "\n",
    "(3) Segmentation TotalVibeSegmentator, Spineps ...\n",
    "\n",
    "(4) Points of Interest (POI) \n",
    "\n",
    "(5) Point registration\n",
    "\n",
    "(6) Deformable Registration for diffrent breath holds compensation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dicom export to BIDS dataset\n",
    "\n",
    "Short overview:\n",
    "\n",
    "A BIDS dataset is a file naming convection.\n",
    "\n",
    "The following rules should be known and weakly enforced:\n",
    "\n",
    "- A dataset folder should start with 'dataset-{YOUR-NAME}'\n",
    "- The next level folder are:\n",
    "  - rawdata: for all imaging data.\n",
    "  - derivative: for all generated data, like segmentation.\n",
    "A file should look like:\n",
    "\n",
    "sub-{Subject name}_ses-{Session}_{key}-{value}*_{format}.{filetype}\n",
    "- Subject name: Unique identifier \n",
    "- Session: Session id. Optional if there is only one session\n",
    "- Any number of key-values. Keys are unique. The defined keys are here: https://bids-specification.readthedocs.io/en/stable/appendices/entities.html . Our tool enforces a certain order. See tutorial_BIDS_files.ipynb\n",
    "- format: type of acquisition like ct, T2w, VIBE, MPRage\n",
    "Do not use '_' in any key or values. \n",
    "\n",
    "See https://bids-specification.readthedocs.io/en/stable/ for detailed description what BIDS ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox.core.bids_files import entities_keys, formats\n",
    "\n",
    "print('Known formats:\\n','; '.join(formats))\n",
    "print()\n",
    "print()\n",
    "print(\"Order of keys we enforce:\\n\", '; '.join(entities_keys.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This function extracts a dicom folder to a BIDS-like Niffty folder.\n",
    "\n",
    "The names are created like this: DICOM:Key is given dicom key\n",
    "      \n",
    "`dataset-{NAME}/rawdate/sub-{DICOM:PatientID}/ses-{DICOM:StudyDate}/{format}/sub-{DICOM:PatientID}_ses-{DICOM:StudyDate}_sequ-{DICOM:SeriesNumber}_acq-{sag|ax|cor|iso}_{format}.nii.gz`\n",
    "\n",
    "\n",
    "and a .json, where the and DICOM-Keys are saved.\n",
    "\n",
    "To get {format} we use string matching and the dicom \"SeriesDescription\" key. As this is a free text this will not always work. Than we default to \"mr\" and you have to manually rename them.\n",
    "\n",
    "\n",
    "For very large dataset you can use make_subject_chunks = n [int]. Than we put a additional folder with the first n letters between rawdata and the sub- folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from TPTBox.segmentation.TotalVibeSeg.auto_download import _download\n",
    "\n",
    "path_to_dicom_dataset = Path(\"PixelPandemonium\").absolute()\n",
    "target_folder = Path(path_to_dicom_dataset).parent\n",
    "\n",
    "if not path_to_dicom_dataset.exists():\n",
    "    _download('https://github.com/robert-graf/TotalVibeSegmentator/releases/download/example/PixelPandemonium.zip',path_to_dicom_dataset, text=\"example\")\n",
    "\n",
    "dataset_name = Path(path_to_dicom_dataset).name.replace(\"_\",\"-\") # TODO Remove\n",
    "dataset = target_folder / f\"dataset-{dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox.core.dicom.dicom_extract import extract_dicom_folder\n",
    "\n",
    "extract_dicom_folder(Path(path_to_dicom_dataset), dataset,use_session=True,n_cpu=10,skip_localizer=True)\n",
    "\n",
    "## Example for https://data.mendeley.com/datasets/k57fr854j2/2\n",
    "#extract_dicom_folder(Path(path_to_dicom_dataset), dataset,use_session=True,n_cpu=10,override_subject_name=lambda x,y: y.name.split(\"_\")[-2],skip_localizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tool that automat scans Bids folders an creates a grouped dictionary, where you can pick out the relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import BIDS_FILE, BIDS_Global_info\n",
    "from TPTBox.core.bids_constants import sequence_splitting_keys\n",
    "\n",
    "print(\"if one of the values of these keys is diffrent, than it is considered a other sequence:\", sequence_splitting_keys)\n",
    "print(\"sub will alway split\")\n",
    "\n",
    "print(\"Lets search for candidate for merging. For this we have to remove the sequ-key from sequence_splitting_keys\")\n",
    "my_splitting_keys = sequence_splitting_keys.copy()\n",
    "my_splitting_keys.remove(\"sequ\")\n",
    "my_splitting_keys.append(\"part\")\n",
    "\n",
    "bgi = BIDS_Global_info(dataset,[\"rawdata\",\"derivative\"],sequence_splitting_keys=my_splitting_keys)\n",
    "stitching_candidate:list[BIDS_FILE] = []\n",
    "epsilon = 0.2\n",
    "for name, subj in bgi.iter_subjects():\n",
    "    print('Subject identifier',name)\n",
    "    q = subj.new_query()\n",
    "    #Filter by some rules\n",
    "    q.flatten()\n",
    "    q.filter_filetype('nii.gz')\n",
    "    q.unflatten()\n",
    "    for fam in q.loop_dict():\n",
    "        print(fam)\n",
    "        for key, file_list in fam.items():\n",
    "            if key == \"mr\":\n",
    "                continue\n",
    "            if len(file_list) == 1:\n",
    "                continue\n",
    "            # This code is only an example, where we group images with the same orientation and zoom, so we know what are potential stitching targets.\n",
    "            # We use _format key as the initial split, so T1w and T2w will not be stiched\n",
    "            matching_group = []\n",
    "            for files in range(len(file_list)):\n",
    "                f1 = file_list[files]\n",
    "                if f1 is None:\n",
    "                    continue\n",
    "                grid1 = f1.get_grid_info()\n",
    "                if grid1 is None:\n",
    "                    continue\n",
    "                current_group = [f1]  # Start a new group with the current file\n",
    "                for j in range(files + 1, len(file_list)):\n",
    "                    f2 = file_list[j]\n",
    "                    if f2 is None:\n",
    "                        continue\n",
    "                    grid2 = f2.get_grid_info()\n",
    "                    if grid2 is None:\n",
    "                        continue\n",
    "                    # Check if orientation matches\n",
    "                    if grid1.orientation == grid2.orientation:\n",
    "                        # Check if zoom is within the tolerance\n",
    "                        zoom_diff = [abs(z1 - z2) for z1, z2 in zip(grid1.zoom, grid2.zoom,strict=False)]\n",
    "                        if all(diff <= epsilon for diff in zoom_diff):\n",
    "                            current_group.append(f2)\n",
    "                            file_list[j] = None # type: ignore\n",
    "                # Add the group if it has more than one file\n",
    "                if len(current_group) > 1:\n",
    "                    stitching_candidate.append(current_group)\n",
    "for files in stitching_candidate:\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Stitching  \n",
    "Torax/Fullbody images are often in chunks. We can stich them with the stitching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from TPTBox import to_nii\n",
    "from TPTBox.stitching import stitching\n",
    "\n",
    "derivative_folder = \"rawdata_stiched\"\n",
    "\n",
    "def process_files(files):\n",
    "    files = sorted(files)  # noqa: PLW2901\n",
    "    sequ: str = (files[0].get(\"sequ\", \"\") + \"-\" if \"sequ\" in files[0].info else \"\") + \"stiched\"  # type: ignore\n",
    "    out_name = files[0].get_changed_path(\"nii.gz\", info={\"sequ\": sequ}, parent=derivative_folder)\n",
    "    if not out_name.exists():\n",
    "        stitching(files, out=out_name, is_seg=False, is_ct=files[0].bids_format == \"ct\", dtype=to_nii(files[0]).dtype,match_histogram=True,)\n",
    "        nii = to_nii(out_name)\n",
    "        nii.apply_crop_(nii.compute_crop())\n",
    "        nii.save(out_name)\n",
    "# Test\n",
    "process_files(stitching_candidate[0])\n",
    "\n",
    "# Execute the loop in parallel using a ProcessPoolExecutor\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    executor.map(process_files, stitching_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, Visualization_Type, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "files = stitching_candidate[0]\n",
    "files = sorted(files)\n",
    "sequ: str = (files[0].get(\"sequ\", \"\") + \"-\" if \"sequ\" in files[0].info else \"\") + \"stiched\"\n",
    "t = files[0].get_changed_path(\"nii.gz\", info={\"sequ\": sequ}, parent=derivative_folder)\n",
    "a = [Snapshot_Frame(to_nii(i).resample_from_to(t), sagittal=True) for i in files]\n",
    "b = Snapshot_Frame(t,sagittal=True)\n",
    "\n",
    "create_snapshot(out_img, [*a,b])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Segmentation  \n",
    "\n",
    "Note: by default we do not install Deep-learning stuff.\n",
    "\n",
    "Install:\n",
    "\n",
    "```pip install SPINEPS ruamel.yaml configargparse```\n",
    "\n",
    "trouble shouting: nnunetv2==2.4.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TotalVibeSegmentator\n",
    "\n",
    "https://arxiv.org/abs/2406.00125\n",
    "\n",
    "https://github.com/robert-graf/TotalVibeSegmentator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import BIDS_FILE\n",
    "from TPTBox.segmentation import run_totalvibeseg\n",
    "\n",
    "# run_totalvibeseg\n",
    "# You can alos use a string/Path if you want to set the path yourself.\n",
    "### Just making in and output path\n",
    "dataset = target_folder / \"dataset-PixelPandemonium\"\n",
    "in_file_dixon = BIDS_FILE(f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/dixon/sub-111168223_ses-20230128_sequ-501-stiched_acq-ax_part-water_dixon.nii.gz\",dataset)\n",
    "out_file_dixon = in_file_dixon.get_changed_path(\"nii.gz\",\"msk\",parent=\"derivative\",info={\"seg\":\"TotalVibeSegmentator\",\"mod\":in_file_dixon.bids_format})\n",
    "####\n",
    "run_totalvibeseg(in_file_dixon,out_file_dixon,override=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "a = Snapshot_Frame(in_file_dixon, segmentation=out_file_dixon, sagittal=True, axial=True,axial_heights=[0.5],ignore_seg_for_centering=True)\n",
    "create_snapshot(out_img, [a])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run_totalvibeseg\n",
    "# You can also use a string/Path if you want to set the path yourself.\n",
    "### Just making in and output path\n",
    "dataset = target_folder / \"dataset-PixelPandemonium\"\n",
    "in_file = BIDS_FILE(f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/T2w/sub-111168223_ses-20230128_sequ-201-stiched_acq-ax_part-inphase_T2w.nii.gz\",dataset)\n",
    "out_file = in_file.get_changed_path(\"nii.gz\",\"msk\",parent=\"derivative\",info={\"seg\":\"TotalVibeSegmentator\",\"mod\":in_file.bids_format})\n",
    "####\n",
    "run_totalvibeseg(in_file,out_file,override=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "a = Snapshot_Frame(in_file, segmentation=out_file, sagittal=True, axial=True,axial_heights=[0.2,.4,0.6],ignore_seg_for_centering=True)\n",
    "create_snapshot(out_img, [a])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spineps\n",
    "\n",
    "Spineps can segment spine images in a instance and semantic mask. Running automatic over a dataset is very opinionated, what to segment. \n",
    "TODO: make a way to manully define output paths\n",
    "\n",
    "https://github.com/Hendrik-code/spineps/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your model is BIDS compliant you can auto run spineps\n",
    "#from TPTBox.segmentation import run_spineps_all\n",
    "#run_spineps_all(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a fitting model:\n",
    "from spineps.models import modelid2folder_instance, modelid2folder_semantic\n",
    "\n",
    "print('Available Semantic Models',modelid2folder_semantic())\n",
    "print('Available Instance Models',modelid2folder_instance())\n",
    "\n",
    "print(modelid2folder_semantic().keys())\n",
    "print(modelid2folder_instance().keys())\n",
    "\n",
    "### Just making in and output path\n",
    "dataset = target_folder / \"dataset-PixelPandemonium\"\n",
    "in_file = BIDS_FILE(f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/T2w/sub-111168223_ses-20230128_sequ-401-stiched_acq-sag_part-inphase_T2w.nii.gz\",dataset)\n",
    "####\n",
    "\n",
    "model_semantic = \"t2w\"\n",
    "model_instance = \"instance\"\n",
    "derivative_name = \"derivative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox.segmentation.spineps import run_spineps_single\n",
    "\n",
    "#With 'ignore_compatibility_issues = True' you can force to run the soft ware\n",
    "out_paths = run_spineps_single(\n",
    "    in_file,\n",
    "    dataset=dataset,\n",
    "    model_semantic=model_semantic,\n",
    "    model_instance=model_instance,\n",
    "    derivative_name=derivative_name,\n",
    "    ignore_compatibility_issues=False,use_cpu=False)\n",
    "print(out_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "a = Snapshot_Frame(in_file, segmentation=out_paths[\"out_vert\"],centroids=out_paths[\"out_ctd\"], sagittal=True)\n",
    "b = Snapshot_Frame(in_file, segmentation=out_paths[\"out_spine\"],centroids=out_paths[\"out_ctd\"], sagittal=True)\n",
    "create_snapshot(out_img, [a,b])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Point of intresst\n",
    "\n",
    "We have a json file format, that can be rescaled like a niffty between loacal spaceses and to global space.\n",
    "\n",
    "The file is human reable. Not the file and numpy start conting with 0, while ITKSnap start conting with 1. You have to suptract one in ITKSnap.\n",
    "\n",
    "Loading a file you get a \"POI\" object. Every entry has two levels. They can bechooen abetrally. For Vertebra it is `Vertebra-ID`, `Point number`.\n",
    "\n",
    "https://doi.org/10.3389/fbioe.2022.862804 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import NII, POI\n",
    "\n",
    "nii_path = in_file\n",
    "nii = NII.load(nii_path,False)\n",
    "# Making a POI object in the same space as an image\n",
    "poi_obj:POI = nii.make_empty_POI()\n",
    "print(f\"{nii =}\")\n",
    "print(f\"{poi_obj=}\")\n",
    "# You can set,read poi Objects like you would accses a 2D Dictonary\n",
    "poi_obj[19,50] = (10,20,30)\n",
    "print('Before rescaling',poi_obj[19,50])\n",
    "# We can use rescale, reorient, resample_from_to like in a nii\n",
    "poi_obj = poi_obj.rescale((0.5,.5,.5))\n",
    "print('After rescaling',poi_obj[19,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common way to fill a POI object is by computing them from a Segmentation\n",
    "\n",
    "Lets compute the center of mass of a instance segmentation. Like from SPINEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import Location, calc_centroids\n",
    "\n",
    "nii_instance_path = in_file #MRI File\n",
    "nii_instance_nii = NII.load(out_paths[\"out_vert\"], seg=True) # Instance mask\n",
    "nii_instance_nii[nii_instance_nii>100] = 0 # Only numbers below 100 are Vertebras\n",
    "\n",
    "# You have to set one of the two keys. The other is coming from the numbers in the segmentation\n",
    "print('First stage value (first_stage)',nii_instance_nii.unique())\n",
    "print('Second stage value (second_stage)',Location.Vertebra_Full.value,'\\n')\n",
    "poi = calc_centroids(nii_instance_nii,second_stage=Location.Vertebra_Full.value)\n",
    "\n",
    "print(poi,'\\n')\n",
    "\n",
    "print(f\"example point {poi[nii_instance_nii.unique()[0],Location.Vertebra_Full.value] =}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, Visualization_Type, create_snapshot\n",
    "\n",
    "out_img = Path(\"out.jpg\")\n",
    "a = Snapshot_Frame(in_file, segmentation=nii_instance_nii,centroids=poi, sagittal=True,only_mask_area=True,visualization_type=Visualization_Type.Maximum_Intensity)\n",
    "create_snapshot(out_img, [a])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Vertebra segmentation we have a rule based pipline to generate points.\n",
    "\n",
    "You need the instance and subregion mask from spineps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from TPTBox import Location, Vertebra_Instance, calc_poi_from_subreg_vert\n",
    "\n",
    "nii_instance_path = out_paths[\"out_vert\"]\n",
    "nii_semantic_path = out_paths[\"out_spine\"]\n",
    "\n",
    "# We can compute the direction ot the Vertebra.\n",
    "subreg_ids = [Location.Vertebra_Corpus,Location.Vertebra_Direction_Inferior,Location.Vertebra_Direction_Right,Location.Vertebra_Direction_Posterior]\n",
    "poi_fixed = calc_poi_from_subreg_vert(nii_instance_path,nii_semantic_path,subreg_id=subreg_ids)\n",
    "# Note: we may need to compute additional points that than are also in the poi file.\n",
    "poi_fixed.extract_subregion_(*subreg_ids)\n",
    "print(poi_fixed,\"\\n\")\n",
    "\n",
    "print(\"L1 Corpus\",poi_fixed[Vertebra_Instance.L1,Location.Vertebra_Corpus],\"\\n\")\n",
    "\n",
    "v = np.array(poi_fixed[Vertebra_Instance.L1,Location.Vertebra_Direction_Inferior]) - np.array(poi_fixed[Vertebra_Instance.L1,Location.Vertebra_Corpus])\n",
    "v /= np.sqrt((v**2).sum())\n",
    "print(\"normal down drection\",v,poi_fixed.orientation,\"\\n\")\n",
    "poi_fixed_slp = poi_fixed.reorient((\"S\",\"L\",\"P\"))\n",
    "v = np.array(poi_fixed_slp[Vertebra_Instance.L1,Location.Vertebra_Direction_Inferior]) - np.array(poi_fixed_slp[Vertebra_Instance.L1,Location.Vertebra_Corpus])\n",
    "v /= np.sqrt((v**2).sum())\n",
    "print(\"normal down drection after reorientation\",v,poi_fixed_slp.orientation,\"\\n\")\n",
    "#Not this is normalizet to image space. If you want the dirction in global space resample to (1,1,1) mm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, Visualization_Type, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(in_file, segmentation=out_paths[\"out_vert\"],centroids=poi_fixed,coronal=True, sagittal=True)\n",
    "b = Snapshot_Frame(in_file, segmentation=out_paths[\"out_spine\"],centroids=poi_fixed, sagittal=True)\n",
    "create_snapshot(out_img, [a,b])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the points described in \n",
    "\n",
    "\"Validation of a Patient-Specific Musculoskeletal Model for Lumbar Load Estimation Generated by an Automated Pipeline From Whole Body CT\"\n",
    "\n",
    "https://doi.org/10.3389/fbioe.2022.862804 \n",
    "\n",
    "![Special POIs](https://www.frontiersin.org/files/Articles/862804/fbioe-10-862804-HTML/image_m/fbioe-10-862804-g002.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "new_pois = [\n",
    "    Location.Vertebra_Corpus,\n",
    "    Location.Additional_Vertebral_Body_Middle_Inferior_Median,\n",
    "    Location.Additional_Vertebral_Body_Middle_Superior_Median,\n",
    "    Location.Additional_Vertebral_Body_Posterior_Central_Median,\n",
    "    Location.Additional_Vertebral_Body_Anterior_Central_Median,\n",
    "    Location.Ligament_Attachment_Point_Anterior_Longitudinal_Superior_Median,\n",
    "    Location.Ligament_Attachment_Point_Posterior_Longitudinal_Superior_Median,\n",
    "    Location.Ligament_Attachment_Point_Anterior_Longitudinal_Inferior_Median,\n",
    "    Location.Ligament_Attachment_Point_Posterior_Longitudinal_Inferior_Median,\n",
    "    ]\n",
    "\n",
    "poi_fixed = calc_poi_from_subreg_vert(nii_instance_path,nii_semantic_path,subreg_id=[Location.Vertebra_Direction_Inferior,*new_pois]).extract_subregion(*new_pois)\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(in_file, segmentation=out_paths[\"out_vert\"],centroids=poi_fixed, sagittal=True)\n",
    "b = Snapshot_Frame(in_file, segmentation=out_paths[\"out_spine\"],centroids=poi_fixed, sagittal=True)\n",
    "create_snapshot(out_img, [a,b])\n",
    "Image(filename=out_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Point registration\n",
    "\n",
    "We can use two POI object to do ridghed point registation. Only point are considert that exist in both POIs. \n",
    "\n",
    "\n",
    "We recomed for spine registation atleast two points per Vertebra, to prevent roation around the spine. See https://doi.org/10.1186/s41747-023-00385-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TPTBox import NII, Location, Vertebra_Instance, calc_poi_from_subreg_vert\n",
    "from TPTBox.registration import Point_Registration, ridged_points_from_poi\n",
    "from TPTBox.segmentation.TotalVibeSeg import extract_vertebra_bodies_from_totalVibe\n",
    "\n",
    "# Example registrate axial and sagittal with points.\n",
    "# T2w axial points are computed from the totalvibe segmentor.\n",
    "dataset = target_folder / \"dataset-PixelPandemonium\"\n",
    "\n",
    "nii_instance_path2 = out_paths[\"out_vert\"]\n",
    "nii_semantic_path2 = out_paths[\"out_spine\"]\n",
    "# We recommend if you registrade spineps points too use at least two points.\n",
    "poi_fixed = calc_poi_from_subreg_vert(nii_instance_path,nii_semantic_path,subreg_id=[Location.Vertebra_Corpus,Location.Spinosus_Process]).round(1)\n",
    "fixed_image = BIDS_FILE(f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/T2w/sub-111168223_ses-20230128_sequ-401-stiched_acq-sag_part-inphase_T2w.nii.gz\",dataset)\n",
    "\n",
    "moving_file = BIDS_FILE(f\"{dataset}/rawdata_stiched/sub-111168223/ses-20230128/dixon/sub-111168223_ses-20230128_sequ-501-stiched_acq-ax_part-water_dixon.nii.gz\",dataset)\n",
    "out_file = moving_file.get_changed_path(\"nii.gz\",\"msk\",parent=\"derivative\",info={\"seg\":\"TotalVibeSegmentator\",\"mod\":moving_file.bids_format})\n",
    "moving_image = to_nii(moving_file)\n",
    "out_file = to_nii(out_file,True)\n",
    "\n",
    "#### Let's move the image, so we see better that the registration is working.\n",
    "moving_image.origin = (moving_image.origin[0],moving_image.origin[1],moving_image.origin[2]+25)\n",
    "out_file.origin = moving_image.origin\n",
    "####\n",
    "\n",
    "# Ensure that we count the same as in the T2w sagittal\n",
    "num_thoracic_verts = 12\n",
    "if Vertebra_Instance.T13.value in poi_fixed.keys_region():\n",
    "    num_thoracic_verts = 13\n",
    "if Vertebra_Instance.T12.value not in poi_fixed.keys_region():\n",
    "    num_thoracic_verts = 11\n",
    "num_lumbar_verts= 5\n",
    "if Vertebra_Instance.L6.value in poi_fixed.keys_region():\n",
    "    num_lumbar_verts = 6\n",
    "if Vertebra_Instance.L5.value not in poi_fixed.keys_region():\n",
    "    num_lumbar_verts = 4\n",
    "# Note: this function currently assumes that we see the sacrum in the image.\n",
    "nii, poi_moving = extract_vertebra_bodies_from_totalVibe(out_file,num_lumbar_verts=num_lumbar_verts,num_thoracic_verts=num_thoracic_verts)\n",
    "\n",
    "registation_object:Point_Registration = ridged_points_from_poi(poi_fixed,poi_moving,c_val=0)\n",
    "\n",
    "# Move image\n",
    "moved_nii = registation_object.transform_nii(moving_image)\n",
    "print(moved_nii,'\\n',moving_image,'\\n')\n",
    "# Move poi\n",
    "moved_poi = registation_object.transform_poi(poi_moving).round(1)\n",
    "print(moved_poi,'\\n',poi_moving,'\\n')\n",
    "# Move image by updating the affine, but not resampe the image\n",
    "moving_nii_affine_only = registation_object.transform_nii_affine_only(moving_image)\n",
    "print(moving_nii_affine_only.shape,moving_nii_affine_only.affine.reshape((-1,)).tolist())\n",
    "print(moving_image.shape,moving_image.affine.reshape((-1,)).tolist(),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(moving_image,segmentation=nii, centroids= poi_moving, sagittal=True)\n",
    "create_snapshot(out_img, [a])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(moving_image.resample_from_to(moved_nii,mode=\"constant\"),centroids=poi_fixed, sagittal=True)\n",
    "b = Snapshot_Frame(moved_nii,centroids=poi_fixed,sagittal=True)\n",
    "c = Snapshot_Frame(fixed_image,centroids=poi_fixed,sagittal=True)\n",
    "create_snapshot(out_img, [a,b,c])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Deformable registration for diffrent breath holds compensation\n",
    "\n",
    "With our point pre-registration, we can use deformable registration to remove breathing.\n",
    "\n",
    "(Our example did not suffer from breathing movement, so we us to similar images from ammos22)\n",
    "\n",
    "this example needs deepali installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from TPTBox import to_nii\n",
    "from TPTBox.registration.deformable import Deformable_Registration\n",
    "\n",
    "moving_nii = to_nii(Path(\"PixelPandemonium/PixelPandemonium/mr_0.nii.gz\").absolute(),False)\n",
    "fixed_nii =  to_nii(Path(\"PixelPandemonium/PixelPandemonium/mr_1.nii.gz\").absolute(),False)\n",
    "\n",
    "\n",
    "reg = Deformable_Registration(fixed_nii,moving_nii,normalize=\"MRI\",device=\"cuda\")\n",
    "\n",
    "moved_nii = reg.transform_nii(moving_nii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(moving_nii.resample_from_to(moved_nii,mode=\"constant\"),sagittal=False,axial=True,axial_heights=[0.5],mode=\"MRI\")\n",
    "b = Snapshot_Frame(moved_nii,sagittal=False,axial=True,axial_heights=[0.5],mode=\"MRI\")\n",
    "c = Snapshot_Frame(fixed_nii.resample_from_to(moved_nii,mode=\"constant\"),sagittal=False,axial=True,axial_heights=[0.5],mode=\"MRI\")\n",
    "create_snapshot(out_img, [a,b,c])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from TPTBox.spine.snapshot2D import Snapshot_Frame, create_snapshot\n",
    "\n",
    "### make snapshot\n",
    "out_img = Path(\"out.jpg\")\n",
    "\n",
    "a = Snapshot_Frame(moving_nii.resample_from_to(moved_nii,mode=\"constant\"),sagittal=True,mode=\"MRI\")\n",
    "b = Snapshot_Frame(moved_nii,sagittal=True,mode=\"MRI\")\n",
    "c = Snapshot_Frame(fixed_nii.resample_from_to(moved_nii,mode=\"constant\"),sagittal=True,mode=\"MRI\")\n",
    "create_snapshot(out_img, [a,b,c])\n",
    "Image(filename=out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
